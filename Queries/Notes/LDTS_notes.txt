LOAD_DTS (load date/time stamp) --
This attribute is part of every Data Vault entity, and is the date/time that drives 
the effective dating in the Satellite/Link Satellite entities(where it is part of the key)

We need to store various different perspectives (date/times) as context -
--The date/time the ETL batch / workflow has started (across all ETL processes)
--The date/time the ETL was processed (ETL process date / Job Run Date/time) – at what time was the 
   ETL executed that did something with the data
--The moment of change in the source, as accurately as possible – closest to the real change made by a user
  (the Event Date/Time concept) – always computed and non-tamperable / derived for instance by database log transactions
--All kinds of date/time stamps as appear in, or are tracked by, the operational system. These are generally modeled 
  as regular context attributes (Satellite or Link-Satellite attributes)
--The moment of insertion in the database – there might be delays in ETL execution and actual inserts. 
	An auditor might want to know when the data was loaded (and changed)


Note**
Information can be presented / delivered following any of the available timelines in the data using Data/Information Marts 
(front-room / Presentation Layer) regardless how it is stored in Data Vault (back room / Integration Layer).

choosing which field needs to be part of key can be challenging, however 1 needs to be chosen.
against which record should I compare my new record?

LOAD_DTS in its original definition-is the execution date/time for the full workflow/batch to load the (entire) Data Vault. 
--we can have a mechanism to create a single execution time to be picked up by every ETL that is part of batch.

The underlying idea is that a LOAD_DTS that is created as batch date/time 
stamp is one of the few fields that are completely under control and thus no dependency on external system
 or smthing out of control (like source dates, or date/time generated by operational systems), helps
avoiding time zone issues.

--limitation: cant run multiple loads with same key.
An ETL process should really be able to load multiple changes (for the same source key) in one go
So, it should be 
Event Date/Time concept driving the effective dates (‘act’ as LOAD_DTS).

The Event Date/Time concept is defined as the immutable (‘non-tamperable’) moment the change is recorded; 
as close as you can get to the ‘real’ event (a record changing in the source). 
The exact Event Date/Time value depends on the interface type; the best option for this needs to
 be selected depending on the available interfacing options.


The LOAD_DTS in its original meaning as insert date/time can still be incorporated, 
but needs to be reconstructed using the available ETL process metadata.

We would want to decouple the ability to loading the ETL with the timelines in the data.

Limitations:
Depending on your technical architecture and (types of) interfaces you may run into time zone issues & related key violation issues
&
experience incorrect order of recorded time. (also in DST)
Think of UTC time application..

#applying time zone business logic into your initial Data Vault interfacing ETL is a very risky approach, 
and is recommended to be avoided..

as per DV 2.0,
LOAD_DTS: the date/time the record was received (inserted by) the database.

this can mean the date/time the record was received by the Staging Area or the Data Vault (or both).

[Option:
‘locking in’ the date/time in the Staging Area so it can be propagated from there. 
The reason is that, it allows to truncate and rebuild the entire Data Vault from a Historical Staging Area in exactly the same way

Nte**a persistent Staging Area of any form is currently not part of the Data Vault definitions as published by Dan.

Two options:
>>>>>If the LOAD_DTS is set into the Staging Area (before loading this into the Data Vault), the following applies:

You can truncate and rebuild your entire Data Vault from an archive (Historical Staging Area), 
and are able to reproduce this in the exact same way (e.g. representing the same LOAD_DTS)
You can prevent loading multiple loads (duplicates) by comparing on the LOAD_DTS you set in the Staging Area. 
If you already lock in the date/time in the Staging Area, you can just check on this value (and the natural key) to prevent reloads
You also may want to log the moment the record is inserted into the Data Vault
In full near Real Time interfaces the Staging Area may be skipped, which obviously means the LOAD_DTS cannot be set on 
the way in to Staging. 

The same LOAD_DTS (for the Staging Area table) is can be consistently applied in all upstream tables that are sourced from 
that Staging Area table


>>>>If the LOAD_DTS is set into the Data Vault (Integration Layer), the following applies:

If you truncate and rebuild your Data Vault from an archive, your effective dates will be different each time you do so
Preventing loading duplicates requires additional smarts. You cannot depend on the LOAD_DTS to prevent reruns, 
so you need additional information to prevent records from being reloaded. 

In normal situations checksum mechanisms can be applied to compare information between new (Staging) and existing/current 
(Data Vault) information. This is typically executed against the most recent record in the Satellite
However, the above only works when a single key delta is processed. When (re)loading multiple changes per natural key 
in a single pass, every record also needs to be compared against… something. 
This may be the Event Date/Time, or a (system) ROW ID, or both
Similar to the previous bullet, if you reload historical data against an existing historical data (e.g. Satellite) set 
you need to be able to do a point in time comparison, for which you need another date/time than the LOAD_DTS (I use the Event Date/Time for this)
Using the pattern this pattern generally allows you to omit Staging (and Historical Staging) areas in your architecture










